{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython import display\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from imageio import imread\n",
    "%matplotlib inline\n",
    "class NN(nn.Module)\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size\n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "        batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])\n",
    "                       ),\n",
    "        batch_size=128, shuffle=True)\n",
    "net = NN(28*28, 1024, 10)\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "def model(x_data, y_data):\n",
    "    \n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "    \n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)\n",
    "softplus = torch.nn.Softplus()\n",
    "def guide(x_data, y_data)\n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight)\n",
    "    outw_sigma = torch.randn_like(net.out.weight)\n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias)\n",
    "    outb_sigma = torch.randn_like(net.out.bias)\n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    return lifted_module()\n",
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())\n",
    "num_iterations = 5\n",
    "loss = 0\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,28*28), data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)\n",
    "num_samples = 10\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.view(-1,28*28))\n",
    "    total += labels.size(0)\n",
    "    dl = np.array(predicted)\n",
    "    gl = np.array(labels)\n",
    "    cl = dl == gl\n",
    "    hl = list(cl)\n",
    "    correct += hl.count(True)    \n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))\n",
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #plt.imshow(npimg,  cmap='gray')\n",
    "    #fig.show(figsize=(1,1)\n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n",
    "    plt.show()\n",
    "num_samples = 100\n",
    "def give_uncertainities(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [F.log_softmax(model(x.view(-1,28*28)).data, 1).detach().numpy() for model in sampled_models]\n",
    "    return np.asarray(yhats)\n",
    "    #mean = torch.mean(torch.stack(yhats), 0)\n",
    "    #return np.argmax(mean, axis=1)\n",
    "def test_batch(images, labels, plot=True):\n",
    "    y = give_uncertainities(images)\n",
    "    predicted_for_images = 0\n",
    "    correct_predictions=0\n",
    "    for i in range(len(labels))\n",
    "        if(plot):\n",
    "            print(\"Real: \",labels[i].item())\n",
    "            fig, axs = plt.subplots(1, 10, sharey=True,figsize=(20,2))\n",
    "    \n",
    "        all_digits_prob = []\n",
    "        highted_something = False\n",
    "        for j in range(len(classes))\n",
    "            highlight=False\n",
    "            histo = []\n",
    "            histo_exp = []\n",
    "            for z in range(y.shape[0]):\n",
    "                histo.append(y[z][i][j])\n",
    "                histo_exp.append(np.exp(y[z][i][j]))\n",
    "            prob = np.percentile(histo_exp, 50) #sampling median probability\n",
    "            if(prob>0.2): #select if network thinks this sample is 20% chance of this being a label\n",
    "                highlight = True #possibly an answer\n",
    "            all_digits_prob.append(prob)\n",
    "            if(plot):\n",
    "                N, bins, patches = axs[j].hist(histo, bins=8, color = \"lightgray\", lw=0,  weights=np.ones(len(histo)) / len(histo), density=False)\n",
    "                axs[j].set_title(str(j)+\" (\"+str(round(prob,2))+\")\") \n",
    "            if(highlight):\n",
    "                highted_something = True\n",
    "                if(plot):\n",
    "                    # We'll color code by height, but you could use any scalar\n",
    "                    fracs = N / N.max()\n",
    "                    # we need to normalize the data to 0..1 for the full range of the colormap\n",
    "                    norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "                    # Now, we'll loop through our objects and set the color of each accordingly\n",
    "                    for thisfrac, thispatch in zip(fracs, patches):\n",
    "                        color = plt.cm.viridis(norm(thisfrac))\n",
    "                        thispatch.set_facecolor(color)\n",
    "        if(plot):\n",
    "            plt.show()\n",
    "        predicted = np.argmax(all_digits_prob)\n",
    "        if(highted_something):\n",
    "            predicted_for_images+=1\n",
    "            if(labels[i].item()==predicted):\n",
    "                if(plot):\n",
    "                    print(\"Correct\")\n",
    "                correct_predictions +=1.0\n",
    "            else:\n",
    "                if(plot):\n",
    "                    print(\"Incorrect :()\")\n",
    "        else:\n",
    "            if(plot):\n",
    "                print(\"Undecided.\")\n",
    "        if(plot):\n",
    "            imshow(images[i].squeeze())\n",
    "    if(plot):\n",
    "        print(\"Summary\")\n",
    "        print(\"Total images: \",len(labels))\n",
    "        print(\"Predicted for: \",predicted_for_images)\n",
    "        print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
    "    return len(labels), correct_predictions, predicted_for_images\n",
    "# Prediction when network can decide not to predict\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "for j, data in enumerate(test_loader):\n",
    "    images, labels = data\n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "print(\"Total images: \", total)\n",
    "print(\"Skipped: \", total-total_predicted_for)\n",
    "print(\"Accuracy when made predictions: %d %%\" % (100 * correct / total_predicted_for))\n",
    "# preparing for evaluation\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "test_batch(images[:100], labels[:100])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
